{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "from tqdm import tqdm\n",
    "import preprocess\n",
    "import prompt\n",
    "import postprocess\n",
    "import evaluation\n",
    "import visualization\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = preprocess.get_all_image_paths('evaluation_dataset/images/case*.png')\n",
    "questions = util.load_json('evaluation_dataset/questions.json')\n",
    "questions_with_choices = util.load_json('evaluation_dataset/questions_with_choices.json')\n",
    "questions_with_choices_explained = util.load_json('evaluation_dataset/questions_with_choices_explained.json')\n",
    "examples = util.load_json('evaluation_dataset/examples.json')[0]\n",
    "with open(\"evaluation_dataset/choices_per_feature.json\", \"r\") as f:\n",
    "    choices_per_feature_freq = json.load(f)\n",
    "with open(\"evaluation_dataset/class_support.json\", \"r\") as f:\n",
    "    class_support = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_pipeline(experiment_name: str):\n",
    "    \"\"\"\n",
    "    The function analyzes the annotations by calculating feature weighted recall,\n",
    "    storing the result and plotting it.\n",
    "    This function can only be used when all annotations are generated as it \n",
    "    automatically reads the annotation files stored in the experiment_name folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_name:\n",
    "        The experiment name (which is also the folder name) which contains the annotations.\n",
    "    \"\"\"\n",
    "    doc_anns, gpt_anns, features = postprocess.extract_annotations(\n",
    "        'evaluation_dataset/doc_anns.json',\n",
    "        f'evaluation_results/{experiment_name}/gpt_anns.json',\n",
    "        f'evaluation_results/{experiment_name}/anns_comparison.xlsx'\n",
    "    )\n",
    "    features_acc = evaluation.compute_feature_accuracies(doc_anns, gpt_anns, features, choices_per_feature_freq, average='weighted')\n",
    "    util.save_as_json(features_acc, f'evaluation_results/{experiment_name}/features_acc.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_bs = 'Zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_bs = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_bs.append(prompt.zeroshot(image_path, query))\n",
    "util.save_as_json(ann_bs, f'evaluation_results/{exp_name_bs}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_bs = util.load_json(f'evaluation_results/{exp_name_bs}/annotations.json')\n",
    "answers_bs = []\n",
    "for ann in anns_bs:\n",
    "    answer_bs = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_bs.append(answer_bs)\n",
    "util.save_as_json(answers_bs, f'evaluation_results/{exp_name_bs}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot_Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_bs = 'Zeroshot_Grok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_bs = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_bs.append(prompt.zeroshot_grok(image_path, query))\n",
    "util.save_as_json(ann_bs, f'evaluation_results/{exp_name_bs}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_bs = util.load_json(f'evaluation_results/{exp_name_bs}/annotations.json')\n",
    "answers_bs = []\n",
    "for ann in anns_bs:\n",
    "    answer_bs = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_bs.append(answer_bs)\n",
    "util.save_as_json(answers_bs, f'evaluation_results/{exp_name_bs}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot_Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_bs = 'Zeroshot_Claude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_bs = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_bs.append(prompt.zeroshot_claude(image_path, query))\n",
    "util.save_as_json(ann_bs, f'evaluation_results/{exp_name_bs}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_bs = util.load_json(f'evaluation_results/{exp_name_bs}/annotations.json')\n",
    "answers_bs = []\n",
    "for ann in anns_bs:\n",
    "    answer_bs = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_bs.append(answer_bs)\n",
    "util.save_as_json(answers_bs, f'evaluation_results/{exp_name_bs}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_var = 'Zeroshot_Var'\n",
    "exp_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "for i in range(exp_num):\n",
    "    ann_var = []\n",
    "    for image_path in tqdm(image_filenames):\n",
    "        ann_var.append(prompt.zeroshot(image_path, query))\n",
    "    util.save_as_json(ann_var, f'evaluation_results/{exp_name_var}/annotations{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(exp_num):\n",
    "    anns_var = util.load_json(f'evaluation_results/{exp_name_var}/annotations{i}.json')\n",
    "    answers_var = []\n",
    "    for ann in anns_var:\n",
    "        answer_var = postprocess.extract_answers(questions_with_choices, ann)\n",
    "        answers_var.append(answer_var)\n",
    "    util.save_as_json(answers_var, f'evaluation_results/{exp_name_var}/gpt_anns{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(exp_num):\n",
    "    doc_anns, gpt_anns, features = postprocess.extract_annotations(\n",
    "        'evaluation_dataset/doc_anns.json',\n",
    "        f'evaluation_results/{exp_name_var}/gpt_anns{i}.json',\n",
    "        f'evaluation_results/{exp_name_var}/annotations_comparison{i}.xlsx'\n",
    "    )\n",
    "\n",
    "    features_acc = evaluation.compute_feature_accuracies(doc_anns, gpt_anns, features, choices_per_feature_freq, average='macro')\n",
    "    util.save_as_json(features_acc, f'evaluation_results/{exp_name_var}/features_acc{i}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot Var Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_var = 'Zeroshot_Var_Explained'\n",
    "exp_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices_explained)\n",
    "for i in range(exp_num):\n",
    "    ann_var = []\n",
    "    for image_path in tqdm(image_filenames):\n",
    "        ann_var.append(prompt.zeroshot(image_path, query))\n",
    "    util.save_as_json(ann_var, f'evaluation_results/{exp_name_var}/annotations{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(exp_num):\n",
    "    anns_var = util.load_json(f'evaluation_results/{exp_name_var}/annotations{i}.json')\n",
    "    answers_var = []\n",
    "    for ann in anns_var:\n",
    "        answer_var = postprocess.extract_answers(questions_with_choices, ann)\n",
    "        answers_var.append(answer_var)\n",
    "    util.save_as_json(answers_var, f'evaluation_results/{exp_name_var}/gpt_anns{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(exp_num):\n",
    "    doc_anns, gpt_anns, features = postprocess.extract_annotations(\n",
    "        'evaluation_dataset/doc_anns.json',\n",
    "        f'evaluation_results/{exp_name_var}/gpt_anns{i}.json',\n",
    "        f'evaluation_results/{exp_name_var}/annotations_comparison{i}.xlsx'\n",
    "    )\n",
    "\n",
    "    features_acc = evaluation.compute_feature_accuracies(doc_anns, gpt_anns, features, choices_per_feature_freq)\n",
    "    util.save_as_json(features_acc, f'evaluation_results/{exp_name_var}/features_acc{i}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot Free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_bsf = 'Zeroshot_free'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_bsf = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_bsf.append(prompt.zeroshot_free(image_path, query))\n",
    "util.save_as_json(ann_bsf, f'evaluation_results/{exp_name_bsf}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_bsf = util.load_json(f'evaluation_results/{exp_name_bsf}/annotations.json')\n",
    "answers_bsf = []\n",
    "for ann in anns_bsf:\n",
    "    answer_bsf = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_bsf.append(answer_bsf)\n",
    "util.save_as_json(answers_bsf, f'evaluation_results/{exp_name_bsf}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_bsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_fs = 'Fewshots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "fewshot_ex = ' '.join(item for item in examples['fewshots'])\n",
    "ann_fs = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_fs.append(prompt.fewshots(image_path, query, fewshot_ex))\n",
    "util.save_as_json(ann_fs, f'evaluation_results/{exp_name_fs}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_fs = util.load_json(f'evaluation_results/{exp_name_fs}/annotations.json')\n",
    "answers_fs = []\n",
    "for ann in anns_fs:\n",
    "    answer_fs = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_fs.append(answer_fs)\n",
    "util.save_as_json(answers_fs, f'evaluation_results/{exp_name_fs}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewshots with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_fsimg = 'FewshotsImg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_fsimg = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_fsimg.append(prompt.fewshotsImg(image_path, query, 'evaluation_dataset/images/case1.png', examples['fewshots'][0], 'evaluation_dataset/images/case5.png', examples['fewshots'][1]))\n",
    "util.save_as_json(ann_fsimg, f'evaluation_results/{exp_name_fsimg}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_fsimg = util.load_json(f'evaluation_results/{exp_name_fsimg}/annotations.json')\n",
    "answers_fsimg = []\n",
    "for ann in anns_fsimg:\n",
    "    answer_fsimg = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_fsimg.append(answer_fsimg)\n",
    "util.save_as_json(answers_fsimg, f'evaluation_results/{exp_name_fsimg}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_fsimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT + Fewshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_cot = 'CoTFewshots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "ann_cot = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_cot.append(prompt.CoTFewshots(image_path, query, cot_ex))\n",
    "util.save_as_json(ann_cot, f'evaluation_results/{exp_name_cot}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_cot = util.load_json(f'evaluation_results/{exp_name_cot}/annotations.json')\n",
    "answers_cot = []\n",
    "for ann in anns_cot:\n",
    "    answer_cot = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_cot.append(answer_cot)\n",
    "util.save_as_json(answers_cot, f'evaluation_results/{exp_name_cot}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToT + Fewshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_tot = 'ToTFewshots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_ex = ' '.join(item for item in examples['cot'])\n",
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_tot = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_tot.append(prompt.ToTFewshots(image_path, query, 3, 5, tot_ex))\n",
    "util.save_as_json(ann_tot, f'evaluation_results/{exp_name_tot}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_tot = util.load_json(f'evaluation_results/{exp_name_tot}/annotations.json')\n",
    "answers_tot = []\n",
    "for ann in anns_tot:\n",
    "    answer_tot = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_tot.append(answer_tot)\n",
    "util.save_as_json(answers_tot, f'evaluation_results/{exp_name_tot}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_var = 'Consistency'\n",
    "exp_num = 5\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "for i in range(exp_num):\n",
    "    ann_var = []\n",
    "    for image_path in tqdm(image_filenames):\n",
    "        ann_var.append(prompt.CoTFewshots(image_path, query, cot_ex))\n",
    "    util.save_as_json(ann_var, f'evaluation_results/{exp_name_var}/annotations{i}.json')\n",
    "for i in range(exp_num):\n",
    "    anns_var = util.load_json(f'evaluation_results/{exp_name_var}/annotations{i}.json')\n",
    "    answers_var = []\n",
    "    for ann in anns_var:\n",
    "        answer_var = postprocess.extract_answers(questions_with_choices, ann)\n",
    "        answers_var.append(answer_var)\n",
    "    util.save_as_json(answers_var, f'evaluation_results/{exp_name_var}/gpt_anns{i}.json')\n",
    "for i in range(exp_num):\n",
    "    doc_anns, gpt_anns, features = postprocess.extract_annotations(\n",
    "        'evaluation_dataset/doc_anns.json',\n",
    "        f'evaluation_results/{exp_name_var}/gpt_anns{i}.json',\n",
    "        f'evaluation_results/{exp_name_var}/annotations_comparison{i}.xlsx'\n",
    "    )\n",
    "\n",
    "    features_acc = evaluation.compute_feature_accuracies(doc_anns, gpt_anns, features, choices_per_feature_freq)\n",
    "    util.save_as_json(features_acc, f'evaluation_results/{exp_name_var}/features_acc{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = []\n",
    "for i in range(25):\n",
    "    consistent_ans = {}\n",
    "    case_answers = []\n",
    "    for j in range(5):\n",
    "        file_path = f\"evaluation_results/Consistency/gpt_anns{j}.json\"\n",
    "        answers = util.load_json(file_path)\n",
    "        case_answer = json.loads(answers[i])\n",
    "        case_answers.append(case_answer)\n",
    "    for item in range(len(case_answers[0])):\n",
    "        values = []\n",
    "        for index in range(5):\n",
    "            values.append(list(case_answers[index].values())[item])\n",
    "            value_counts = Counter(values)\n",
    "            value, _ = value_counts.most_common(1)[0]\n",
    "            consistent_ans[list(case_answers[0].keys())[item]] = value\n",
    "    final_answer.append(json.dumps(consistent_ans))\n",
    "util.save_as_json(final_answer, 'evaluation_results/Consistency/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline('Consistency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_tot = 'Debate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_ex = ' '.join(item for item in examples['debate'])\n",
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_tot = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_tot.append(prompt.self_debate(image_path, query, tot_ex))\n",
    "util.save_as_json(ann_tot, f'evaluation_results/{exp_name_tot}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_tot = util.load_json(f'evaluation_results/{exp_name_tot}/annotations.json')\n",
    "answers_tot = []\n",
    "for ann in anns_tot:\n",
    "    answer_tot = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_tot.append(answer_tot)\n",
    "util.save_as_json(answers_tot, f'evaluation_results/{exp_name_tot}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_tot = 'Critique'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cri_ex = ' '.join(item for item in examples['critique'])\n",
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "ann_tot = []\n",
    "for image_path in tqdm(image_filenames):\n",
    "    ann_tot.append(prompt.self_critique(image_path, query, cri_ex))\n",
    "util.save_as_json(ann_tot, f'evaluation_results/{exp_name_tot}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_tot = util.load_json(f'evaluation_results/{exp_name_tot}/annotations.json')\n",
    "answers_tot = []\n",
    "for ann in anns_tot:\n",
    "    answer_tot = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_tot.append(answer_tot)\n",
    "util.save_as_json(answers_tot, f'evaluation_results/{exp_name_tot}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "query = ' '.join(str(item) for item in questions_with_choices)\n",
    "\n",
    "for temp in np.linspace(0, 0.6, 5):\n",
    "    exp_name_temp = f'Temperature_{temp}'\n",
    "    ann_temp = []\n",
    "    for image_path in tqdm(image_filenames):\n",
    "        ann_temp.append(prompt.CoTFewshots(image_path, query, cot_ex, temperature=temp))\n",
    "    util.save_as_json(ann_temp, f'evaluation_results/{exp_name_temp}/annotations.json')\n",
    "    anns_temp = util.load_json(f'evaluation_results/{exp_name_temp}/annotations.json')\n",
    "    answers_temp = []\n",
    "    for ann in anns_temp:\n",
    "        answer_temp = postprocess.extract_answers(questions_with_choices, ann)\n",
    "        answers_temp.append(answer_temp)\n",
    "    util.save_as_json(answers_temp, f'evaluation_results/{exp_name_temp}/gpt_anns.json')\n",
    "    analysis_pipeline(exp_name_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "for i in range(5):\n",
    "    shf_qs = preprocess.randomized_list(questions_with_choices, 13)\n",
    "    query_qs = ' '.join(str(item) for item in shf_qs)\n",
    "    exp_name_order = f'Order{i}'\n",
    "\n",
    "    anns_order = []\n",
    "    for image_path in tqdm(image_filenames):\n",
    "        anns_order.append(prompt.CoTFewshots(image_path, query_qs, cot_ex))\n",
    "    util.save_as_json(anns_order, f'evaluation_results/{exp_name_order}/annotations.json')\n",
    "\n",
    "    anns_order = util.load_json(f'evaluation_results/{exp_name_order}/annotations.json')\n",
    "    answers_order = []\n",
    "    for ann in anns_order:\n",
    "        answer_order = postprocess.extract_answers(questions_with_choices, ann)\n",
    "        answers_order.append(answer_order)\n",
    "    util.save_as_json(answers_order, f'evaluation_results/{exp_name_order}/gpt_anns.json')\n",
    "\n",
    "    analysis_pipeline(exp_name_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_sg = f'Single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_sg = []\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "for image_path in tqdm(image_filenames):\n",
    "    single_ans = ''\n",
    "    for question in questions_with_choices:\n",
    "        single_ans += prompt.CoTFewshots(image_path, question, cot_ex)\n",
    "    anns_sg.append(single_ans)\n",
    "util.save_as_json(anns_sg, f'evaluation_results/{exp_name_sg}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_sg = util.load_json(f'evaluation_results/{exp_name_sg}/annotations.json')\n",
    "answers_sg = []\n",
    "for ann in anns_sg:\n",
    "    answer_sg = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_sg.append(answer_sg)\n",
    "util.save_as_json(answers_sg, f'evaluation_results/{exp_name_sg}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_loo = f'Leave_one_out'\n",
    "doc_anns = util.load_json('evaluation_dataset/doc_anns.json')\n",
    "questions = preprocess.extract_questions('evaluation_dataset/questions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_loo = []\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "for i, image_path in enumerate(image_filenames):\n",
    "    print(i)\n",
    "    image_path = image_filenames[i]\n",
    "    doc_ann = doc_anns[i]\n",
    "    single_ans = ''\n",
    "    for (question, qa) in tqdm(zip(questions, questions_with_choices)):\n",
    "        doc_ann_copy = doc_ann.copy()\n",
    "        doc_ann_copy.pop(question)\n",
    "        information = \"\\n\".join(f\"{key}{value}\" for key, value in list(doc_ann_copy.items())[:-1])\n",
    "        single_ans += prompt.leave_one_out(image_path, information, qa, cot_ex)\n",
    "    anns_loo.append(single_ans)\n",
    "util.save_as_json(anns_loo, f'evaluation_results/{exp_name_loo}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_loo = util.load_json(f'evaluation_results/{exp_name_loo}/annotations.json')\n",
    "answers_loo = []\n",
    "for ann in anns_loo:\n",
    "    answer_loo = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_loo.append(answer_loo)\n",
    "util.save_as_json(answers_loo, f'evaluation_results/{exp_name_loo}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave one out txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_loo = f'Leave_one_out_txt'\n",
    "doc_anns = util.load_json('evaluation_dataset/doc_anns.json')\n",
    "questions = preprocess.extract_questions('evaluation_dataset/questions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_loo = []\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "for i, image_path in enumerate(image_filenames):\n",
    "    print(i)\n",
    "    image_path = image_filenames[i]\n",
    "    doc_ann = doc_anns[i]\n",
    "    single_ans = ''\n",
    "    for (question, qa) in tqdm(zip(questions, questions_with_choices)):\n",
    "        doc_ann_copy = doc_ann.copy()\n",
    "        doc_ann_copy.pop(question)\n",
    "        information = \"\\n\".join(f\"{key}{value}\" for key, value in list(doc_ann_copy.items())[:-1])\n",
    "        single_ans += prompt.leave_one_out_txt(information, qa, cot_ex)\n",
    "    anns_loo.append(single_ans)\n",
    "util.save_as_json(anns_loo, f'evaluation_results/{exp_name_loo}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_loo = util.load_json(f'evaluation_results/{exp_name_loo}/annotations.json')\n",
    "answers_loo = []\n",
    "for ann in anns_loo:\n",
    "    answer_loo = postprocess.extract_answers(questions_with_choices, ann)\n",
    "    answers_loo.append(answer_loo)\n",
    "util.save_as_json(answers_loo, f'evaluation_results/{exp_name_loo}/gpt_anns.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline(exp_name_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct One Known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_cone = f'Correct_one_known'\n",
    "doc_anns = util.load_json('evaluation_dataset/doc_anns.json')\n",
    "questions = preprocess.extract_questions('evaluation_dataset/questions.json')\n",
    "qa_pairs = util.load_json('evaluation_dataset/questions_with_choices_correct_mistakes.json')\n",
    "correction_rec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_attempts = []\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "anns_cone = []\n",
    "for i, image_path in enumerate(image_filenames):\n",
    "    print(i)\n",
    "    image_path = image_filenames[i]\n",
    "    doc_ann = doc_anns[i]\n",
    "    for question in tqdm(questions):\n",
    "        if question not in correction_rec:\n",
    "            correction_rec[question] = {'total': 1, 'correct': 0}\n",
    "        else:\n",
    "            correction_rec[question]['total'] += 1\n",
    "\n",
    "        doc_ann_copy = doc_ann.copy()\n",
    "\n",
    "        original_answer = doc_ann_copy[question]\n",
    "        fake_ans = util.random_answer(original_answer, qa_pairs[question.replace('\\n', '')])\n",
    "        doc_ann_copy[question] = fake_ans\n",
    "\n",
    "        information = \"\\n\".join(f\"{key} {value}\" for key, value in list(doc_ann_copy.items()))\n",
    "        response = {}\n",
    "        while response == {}:\n",
    "            ans = prompt.correct_one_mistake_known(image_path, information, question, cot_ex)\n",
    "            anns_cone.append(ans)\n",
    "            response = postprocess.extract_answers_specific_question(questions_with_choices, ans, question)\n",
    "            response = postprocess.process_response(response)\n",
    "        print(response)\n",
    "        corrected_answer = next(iter(response.values()))\n",
    "\n",
    "        if corrected_answer == original_answer:\n",
    "            correction_rec[question]['correct'] += 1\n",
    "\n",
    "        correction_attempts.append(f\"Question: {question}, Correct answer: {original_answer}, Fake answer: {fake_ans}, Answer after GPT correction: {corrected_answer}\")\n",
    "\n",
    "\n",
    "util.save_as_json(correction_attempts, f'evaluation_results/{exp_name_cone}/correction_attempts.json')\n",
    "util.save_as_json(correction_rec, f'evaluation_results/{exp_name_cone}/correction_rec.json')\n",
    "util.save_as_json(anns_cone, f'evaluation_results/{exp_name_cone}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_result = util.load_json(f'evaluation_results/{exp_name_cone}/correction_rec.json')\n",
    "acc_after_corr = evaluation.compute_feature_acc_from_dict(correction_result)\n",
    "util.save_as_json(acc_after_corr, f'evaluation_results/{exp_name_cone}/features_acc.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct One Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import sparse_embed, sparse_cosine_similarity\n",
    "embedded = {}\n",
    "def is_match(pred: str, true: str) -> bool:\n",
    "        if pred not in embedded:\n",
    "            embedded[pred] = sparse_embed(pred)\n",
    "        if true not in embedded:\n",
    "            embedded[true] = sparse_embed(true)\n",
    "        return sparse_cosine_similarity(embedded[pred], embedded[true]) >= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_cone = f'Correct_one_unknown'\n",
    "doc_anns = util.load_json('evaluation_dataset/doc_anns.json')\n",
    "questions = preprocess.extract_questions('evaluation_dataset/questions.json')\n",
    "qa_pairs = util.load_json('evaluation_dataset/questions_with_choices_correct_mistakes.json')\n",
    "correction_rec = {}\n",
    "questions_loc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_attempts = []\n",
    "anns_cone = []\n",
    "identified_questions = []\n",
    "cot_ex = ' '.join(item for item in examples['cot'])\n",
    "\n",
    "for i, image_path in enumerate(image_filenames):\n",
    "    print(i)\n",
    "    image_path = image_filenames[i]\n",
    "    doc_ann = doc_anns[i]\n",
    "    for question in tqdm(questions):\n",
    "        if question not in correction_rec:\n",
    "            correction_rec[question] = {'total': 1, 'correct': 0}\n",
    "        else:\n",
    "            correction_rec[question]['total'] += 1\n",
    "\n",
    "        if question not in questions_loc:\n",
    "            questions_loc[question] = {'total': 1, 'correct': 0}\n",
    "        else:\n",
    "            questions_loc[question]['total'] += 1\n",
    "\n",
    "        doc_ann_copy = doc_ann.copy()\n",
    "\n",
    "        original_answer = doc_ann_copy[question]\n",
    "        fake_ans = util.random_answer(original_answer, qa_pairs[question.replace('\\n', '')])\n",
    "        doc_ann_copy[question] = fake_ans\n",
    "\n",
    "        information = \"\\n\".join(f\"{key} {value}\" for key, value in list(doc_ann_copy.items()))\n",
    "        response = {}\n",
    "        ans = prompt.correct_one_mistake_unknown(image_path, information, cot_ex)\n",
    "        anns_cone.append(ans)\n",
    "        response = postprocess.extract_answers_specific_question(questions_with_choices, ans, question)\n",
    "        response = postprocess.process_response(response)\n",
    "        identified_question = postprocess.question_locate(questions_with_choices, ans)\n",
    "\n",
    "        identified_questions.append(identified_question)\n",
    "        if is_match(identified_question, question):\n",
    "            questions_loc[question]['correct'] += 1\n",
    "            \n",
    "        print(response)\n",
    "        corrected_answer = next(iter(response.values()))\n",
    "\n",
    "        if corrected_answer == original_answer:\n",
    "            correction_rec[question]['correct'] += 1\n",
    "\n",
    "        correction_attempts.append(f\"Question: {question}, Correct answer: {original_answer}, Fake answer: {fake_ans}, Answer after GPT correction: {corrected_answer}\")\n",
    "\n",
    "\n",
    "util.save_as_json(correction_attempts, f'evaluation_results/{exp_name_cone}/correction_attempts.json')\n",
    "util.save_as_json(correction_rec, f'evaluation_results/{exp_name_cone}/correction_rec.json')\n",
    "util.save_as_json(questions_loc, f'evaluation_results/{exp_name_cone}/questions_loc.json')\n",
    "util.save_as_json(anns_cone, f'evaluation_results/{exp_name_cone}/annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_result = util.load_json(f'evaluation_results/{exp_name_cone}/correction_rec.json')\n",
    "acc_after_corr = evaluation.compute_feature_acc_from_dict(correction_result)\n",
    "id_result = util.load_json(f'evaluation_results/{exp_name_cone}/questions_loc.json')\n",
    "question_identify_rate = evaluation.compute_feature_acc_from_dict(id_result)\n",
    "util.save_as_json(question_identify_rate, f'evaluation_results/{exp_name_cone}/q_id_rate.json')\n",
    "util.save_as_json(acc_after_corr, f'evaluation_results/{exp_name_cone}/features_acc.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Visalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.comparison_star_plot([], f'evaluation_results/plots/Major.png', 'Majority Class Baseline', class_support, figsize=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = ['Zeroshot']\n",
    "features_accs = []\n",
    "for exp in exp_names:\n",
    "    file_path = os.path.join(f'evaluation_results/{exp}/', \"features_acc.json\")\n",
    "    feature_acc = util.load_json(file_path)\n",
    "    features_accs.append(feature_acc)\n",
    "visualization.comparison_star_plot(None, f'evaluation_results/plots/{exp_names[0]}.png', ['Marjority-Class'], class_support, figsize=(12,10), title='Majority Baseline Weighted Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Var Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_accs = []\n",
    "for exp in exp_names:\n",
    "    file_path = os.path.join(f'evaluation_results/Zeroshot_Var/', \"features_acc0.json\")\n",
    "    feature_acc = util.load_json(file_path)\n",
    "    features_accs.append(feature_acc)\n",
    "visualization.comparison_star_plot(features_accs, f'evaluation_results/plots/test.png', exp_names, class_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_accs = []\n",
    "for i in range(5):\n",
    "    feature_acc = util.load_json(f'evaluation_results/Zeroshot_Var_explained/features_acc{i}.json')\n",
    "    feature_accs.append(feature_acc)\n",
    "visualization.create_star_variance(feature_accs, f'evaluation_results/plots/Zeroshot_Var_Explained.png', class_support, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = ['Leave_one_out', 'Leave_one_out_txt']\n",
    "doc_anns = util.load_json('evaluation_dataset/doc_anns.json') # List of 25 dicts\n",
    "path = f\"evaluation_results/question_compare/q_{'+'.join(experiment_names)}.png\"\n",
    "visualization.question_lables_distribution('evaluation_dataset/doc_anns.json', experiment_names, 12, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong Question Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = ['Wrong Answers Location Accuracy', 'Wrong Answers Correction Accuracy']\n",
    "file_path = os.path.join(f'evaluation_results/Correct_one_unknown/', \"q_id_rate.json\")\n",
    "question_acc = util.load_json(file_path)\n",
    "question_acc_list = [question_acc, feature_acc]\n",
    "visualization.comparison_star_plot(question_acc_list, f'evaluation_results/plots/{exp_names[0]}.png', exp_names, None, figsize=(12,10), title='Wrong Answers Location Accuracy VS Wrong Answers Correction Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Recall Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Debate'\n",
    "scores = util.load_json(f'evaluation_results/{exp_name}/features_acc.json')\n",
    "tot = 0\n",
    "for item in scores.items():\n",
    "    tot += item[1]\n",
    "avg = tot / len(scores)\n",
    "print(avg)\n",
    "\n",
    "exp_name = 'Fewshots'\n",
    "scores = util.load_json(f'evaluation_results/{exp_name}/features_acc.json')\n",
    "tot = 0\n",
    "for item in scores.items():\n",
    "    tot += item[1]\n",
    "avg = tot / len(scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
