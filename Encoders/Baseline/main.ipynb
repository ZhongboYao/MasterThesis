{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('evaluation_dataset/lesion_dataset.csv')\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LesionDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, image_processor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.image_processor:\n",
    "            image = self.image_processor(image, return_tensors='pt')['pixel_values'].squeeze(0) # Squeeze out the batch dim.\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, num_classes):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(dataset)\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for test_idx in tqdm(range(n_samples), desc='LOOCV Folds', leave=True):\n",
    "    print(f'LOOCV Fold {test_idx+1}/{n_samples}')\n",
    "    \n",
    "    train_indices = [i for i in range(n_samples) if i != test_idx]\n",
    "    test_indices = [test_idx]\n",
    "    \n",
    "    train_images = dataset.loc[train_indices, 'X'].tolist()\n",
    "    train_labels = dataset.loc[train_indices, 'labels'].tolist()\n",
    "    train_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "    test_images = dataset.loc[test_indices, 'X'].tolist()\n",
    "    test_labels = dataset.loc[test_indices, 'labels'].tolist()\n",
    "    test_labels = label_encoder.transform(test_labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    \n",
    "    train_dataset = LesionDataset(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        image_processor\n",
    "    )\n",
    "    test_dataset = LesionDataset(\n",
    "        test_images,\n",
    "        test_labels,\n",
    "        image_processor\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize model with MLP head\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        'google/vit-base-patch16-224',\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = MLPHead(in_features=in_features, hidden_size=256, num_classes=num_classes)\n",
    "    \n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    \n",
    "    num_epochs = 5\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            predictions.append(predicted.item())\n",
    "            true_labels.append(labels.item())\n",
    "        print(f'Evaluation: True label: {true_labels[-1]}, Predicted label: {predictions[-1]}')\n",
    "    del model, optimizer, train_dataloader, test_dataloader\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "print(f'LOOCV Accuracy: {100 * accuracy:.2f}%')\n",
    "\n",
    "# Decode predictions\n",
    "decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "decoded_true_labels = label_encoder.inverse_transform(true_labels)\n",
    "print(\"Sample predictions:\", decoded_predictions)\n",
    "print(\"Sample true labels:\", decoded_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
