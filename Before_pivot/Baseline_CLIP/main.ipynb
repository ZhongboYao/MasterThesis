{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import api\n",
    "from langchain_qdrant import FastEmbedSparse\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chatbot\n",
    "import retrieval\n",
    "import vector_store as vs\n",
    "import util\n",
    "import evaluation as eval\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import util\n",
    "import api\n",
    "import os\n",
    "\n",
    "import pdf_extraction as pdf\n",
    "import caption_generation as cap\n",
    "import vector_store as vs\n",
    "import retrieval\n",
    "import chatbot\n",
    "\n",
    "from langchain_qdrant import FastEmbedSparse\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "from tqdm import tqdm\n",
    "import chunking\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "import numpy as np\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "import evaluation\n",
    "\n",
    "pdf_paths = [\"Dataset/File1.pdf\", \"Dataset/File2.pdf\", \"Dataset/File3.pdf\", \"Dataset/File4.pdf\"]\n",
    "extract_folder = \"robust_extracted_content\"\n",
    "singlepage_folder = \"single_pages\"\n",
    "evaluation_data_folder = \"evaluation_dataset\"\n",
    "evaluation_result_folder = \"evaluation_results\"\n",
    "\n",
    "text_collection_name = 'texts_CLIP'\n",
    "caption_collection_name = 'captions_CLIP'\n",
    "image_collection_name = 'images_CLIP'\n",
    "\n",
    "txt_similarity_topk = 5\n",
    "cos_filtering_threshold = 0.8\n",
    "cos_filtering_topk = 3\n",
    "image_similarity_topk = 5\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "query = \"query.txt\"\n",
    "evaluation_data = \"complete_evaluation_dataset.json\"\n",
    "eval_result = \"score.json\"\n",
    "eval_record = \"record.json\"\n",
    "text_pred = \"text_predictions.json\"\n",
    "image_pred = \"image_predictions.json\"\n",
    "context = \"retrieved_contexts.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_extracted_content is ready for new content.\n",
      "single_pages is ready for new content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages of file Dataset/File1.pdf: 100%|██████████| 3/3 [00:00<00:00, 93.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files are combined in: manual_extracted_content/File1/File1_Pages.txt\n",
      "Data written to manual_extracted_content/File1/File1_imagesInfo.json\n",
      "Converting pdf pages to individual images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:00,  5.00it/s]\n",
      "Processing pages of file Dataset/File2.pdf: 100%|██████████| 14/14 [00:00<00:00, 309.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files are combined in: manual_extracted_content/File2/File2_Pages.txt\n",
      "Data written to manual_extracted_content/File2/File2_imagesInfo.json\n",
      "Converting pdf pages to individual images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:03,  3.81it/s]\n",
      "Processing pages of file Dataset/File3.pdf: 100%|██████████| 56/56 [00:00<00:00, 576.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files are combined in: manual_extracted_content/File3/File3_Pages.txt\n",
      "Data written to manual_extracted_content/File3/File3_imagesInfo.json\n",
      "Converting pdf pages to individual images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [00:14,  3.94it/s]\n",
      "Processing pages of file Dataset/File4.pdf: 100%|██████████| 20/20 [00:00<00:00, 590.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files are combined in: manual_extracted_content/File4/File4_Pages.txt\n",
      "Data written to manual_extracted_content/File4/File4_imagesInfo.json\n",
      "Converting pdf pages to individual images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:04,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "util.clear_output_folder(extract_folder)\n",
    "util.clear_output_folder(singlepage_folder)\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    file = pdf.PDF(pdf_path)\n",
    "    file.extract_images_and_text(singlepage_folder, extract_folder)\n",
    "    file.save_imageInfo(extract_folder)\n",
    "    file.convert_to_images(singlepage_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_path in pdf_paths:\n",
    "    print(f'Generating captions for {pdf_path}.')\n",
    "    file = pdf.PDF(pdf_path)\n",
    "    file.images = util.create_class_from_json(pdf.Image, f\"{extract_folder}/{file.file_name}/{file.file_name}_imagesInfo.json\")\n",
    "    file.append_images_contexts()\n",
    "    for image in tqdm(file.images, desc='Generating captions for images.'):\n",
    "        image.generate_caption(singlepage_folder)\n",
    "    file.save_imageInfo(extract_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate & Store Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text chunks created: 2\n",
      "Total text chunks created: 16\n",
      "Total text chunks created: 54\n",
      "Total text chunks created: 14\n",
      "Deleted old version collection texts_CLIP\n",
      "Collection texts_CLIP initialized.\n",
      "Deleted old version collection captions_CLIP\n",
      "Collection captions_CLIP initialized.\n",
      "Deleted old version collection images_CLIP\n",
      "Collection images_CLIP initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding captions and images to the vectore store.: 100%|██████████| 98/98 [07:31<00:00,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "images = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    file = pdf.PDF(pdf_path)\n",
    "    file.images = util.create_class_from_json(pdf.Image, f\"{extract_folder}/{file.file_name}/{file.file_name}_imagesInfo.json\")\n",
    "\n",
    "    splits = chunking.naive_chunk(chunk_size, chunk_overlap, f\"{extract_folder}/{file.file_name}/{file.file_name}_Pages.txt\")\n",
    "    for split in splits:\n",
    "        chunks.append(chunking.Chunk(split))\n",
    "\n",
    "    for image in file.images:\n",
    "        images.append(image)\n",
    "\n",
    "vs.create_collection(text_collection_name, 1536)\n",
    "vs.create_collection(caption_collection_name, 1536)\n",
    "vs.create_collection(image_collection_name, 512)\n",
    "\n",
    "# for chunk in tqdm(chunks, desc=\"Adding text cunks to the vectore store.\"):\n",
    "#     vs.add_chunk(text_collection_name, chunk)\n",
    "for image in tqdm(images, desc=\"Adding captions and images to the vectore store.\"):\n",
    "    vs.add_caption(caption_collection_name, image)\n",
    "    vs.add_image(image_collection_name, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respond Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_text_query(\n",
    "        query, \n",
    "        text_collection_name, \n",
    "        caption_collection_name, \n",
    "        txt_similarity_topk, \n",
    "        image_similarity_topk, \n",
    "        cos_filtering_threshold, \n",
    "        cos_filtering_topk,\n",
    "        verbose=1\n",
    "        ):\n",
    "    \n",
    "    dense_embedding_function = OpenAIEmbeddings(api_key=api.OPENAI_KEY, model=\"text-embedding-3-small\")\n",
    "    sparse_embedding_function = FastEmbedSparse(model_name=\"prithivida/Splade_PP_en_v1\")\n",
    "    \n",
    "    collection_text = vs.get_collection(\n",
    "        text_collection_name, \n",
    "        dense_embedding_function,\n",
    "        sparse_embedding_function\n",
    "    )\n",
    "\n",
    "    collection_caption = vs.get_collection(\n",
    "        caption_collection_name, \n",
    "        dense_embedding_function,\n",
    "        sparse_embedding_function\n",
    "    )\n",
    "\n",
    "    text_retriever = retrieval.Retriever(query, collection_text)\n",
    "    text_retriever.similarity_search(txt_similarity_topk)\n",
    "    text_retriever.rerank('content')\n",
    "    text_retriever.cos_filtering(vs.dense_embed, 'content', cos_filtering_threshold, cos_filtering_topk)\n",
    "\n",
    "    context = \" \"\n",
    "    for filtered_context in text_retriever.filtered_contexts:\n",
    "        context += filtered_context\n",
    "    \n",
    "    symptom_explanation = chatbot.symptom_list_response(query, context)\n",
    "\n",
    "    image_retriever = retrieval.Retriever(symptom_explanation, collection_caption)\n",
    "    image_retriever.similarity_search(image_similarity_topk)\n",
    "    image_retriever.rerank('caption')\n",
    "    retrieved_image = image_retriever.reranked_docs[0]\n",
    "    image_payload = vs.retrieve_payload(retrieved_image, collection_caption)\n",
    "    image_path = image_payload[\"image_path\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(symptom_explanation)\n",
    "        util.show_image(image_path)\n",
    "\n",
    "    return symptom_explanation, image_path, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = util.read_query(query).replace('\\n', \"\")\n",
    "respond_text_query(query, text_collection_name, caption_collection_name, txt_similarity_topk, image_similarity_topk, cos_filtering_threshold, cos_filtering_topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respond Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_image_query(\n",
    "        query\n",
    "        ):\n",
    "    \n",
    "    client = QdrantClient(url=api.QDRANT_URL, api_key=api.QDRANT_API)\n",
    "    \n",
    "    model = SentenceTransformer('clip-ViT-B-32', device='cpu')\n",
    "    image_content = Image.open(query)\n",
    "    query_embedding = model.encode(image_content)\n",
    "\n",
    "    found_docs = client.search(\n",
    "        collection_name=image_collection_name,\n",
    "        query_vector=query_embedding,\n",
    "    )\n",
    "\n",
    "    return found_docs[0].payload['caption']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptom: Florid cemento-osseous dysplasia (mixed)\n",
      "\n",
      "Description: Florid cemento-osseous dysplasia is a benign condition involving multiple quadrants of the jaws, often bilaterally and symmetrically. It is associated with the apices of the teeth and is a diffuse form of periapical cemental dysplasia. Radiographically, it undergoes three stages: starting as a radiolucent lesion, progressing to radiopacities within the apical radiolucencies, and finally appearing as a densely radiopaque lesion surrounded by a thin radiolucent line. Adjacent teeth are typically unaffected, stable, and not resorbed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/cdx2g5bx1nd502008w3zn6x80000gn/T/ipykernel_87437/2533335915.py:13: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  found_docs = client.search(\n"
     ]
    }
   ],
   "source": [
    "query = util.read_query('query_image.txt')\n",
    "respond_image_query(query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content from evaluation_dataset/evaluation_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/cdx2g5bx1nd502008w3zn6x80000gn/T/ipykernel_35557/646882139.py:11: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  found_docs = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to evaluation_results/predictions.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_data = util.load_json('evaluation_dataset/evaluation_data.json')\n",
    "results = []\n",
    "for entry in evaluation_data:\n",
    "    query = entry['image_path']\n",
    "    retrieved_caption = respond_image_query(query)\n",
    "    entry['caption_retrieved'] = retrieved_caption\n",
    "    results.append(entry)\n",
    "util.save_as_json(results, 'evaluation_results/predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content from evaluation_results/predictions.json\n",
      "Saved content to evaluation_results/predictions.json\n"
     ]
    }
   ],
   "source": [
    "predicts = util.load_json('evaluation_results/predictions.json')\n",
    "for predict in predicts:\n",
    "    if predict['caption_truth'] == predict['caption_retrieved']:\n",
    "        predict['correct'] = 1\n",
    "    else:\n",
    "        predict['correct'] = 0\n",
    "util.save_as_json(predicts, 'evaluation_results/predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
