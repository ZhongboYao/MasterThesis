{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import fitz\n",
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from glob import glob\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import chromadb\n",
    "import uuid\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display\n",
    "import openai\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from llama_index.core.evaluation.multi_modal import MultiModalRelevancyEvaluator, MultiModalFaithfulnessEvaluator\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from IPython.display import Image, display\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from fastembed import SparseTextEmbedding\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import sys\n",
    "import pyocr\n",
    "import pyocr.builders\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import clip  # assuming you're using OpenAI's CLIP package\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = ['Dataset/File1.pdf', 'Dataset/File2.pdf', 'Dataset/File3.pdf', 'Dataset/File4.pdf']\n",
    "\n",
    "output_folder = 'extracted_content/'\n",
    "pdf_img_folder = 'pdf_images'\n",
    "text_output_file = f'{output_folder}/pages.txt'\n",
    "caption_save_path = f'{output_folder}/caption_pairs.json'\n",
    "\n",
    "chunk_size = 50\n",
    "chunk_overlap = 20\n",
    "\n",
    "text_collection_name = 'CLIP_texts'\n",
    "image_collection_name = 'CLIP_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = None\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "qdrant_client = QdrantClient(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts & Images Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_output_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f'Existing folder \"{folder_path}\" and its contents have been removed.')\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    print(f'Folder \"{folder_path}\" is ready for new content.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(txt_file, page, file_index, page_num, pdf_img_folder):\n",
    "    text = page.get_text()\n",
    "    lines = text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip()]\n",
    "    cleaned_text = '\\n'.join(non_empty_lines)\n",
    "    txt_file.write(cleaned_text)\n",
    "    txt_file.write(\"\\n\\n\")\n",
    "\n",
    "    single_page_file = f\"{pdf_img_folder}/File{file_index}_Page{page_num+1}.txt\"\n",
    "    with open(single_page_file, \"a\", encoding=\"utf-8\") as single_txt:\n",
    "        single_txt.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(doc, page_num, page, img_index, file_index, img_metas):\n",
    "    image_list = page.get_images(full=True)\n",
    "\n",
    "    if image_list:\n",
    "        for _, img in enumerate(image_list, start=1):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"File{file_index}_Figure{img_index}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            img_index += 1\n",
    "            img_metas.append([image_filename, page_num])\n",
    "    \n",
    "    else:\n",
    "        print(f\"No images found on page {page_num + 1}.\")\n",
    "\n",
    "    return img_index, img_metas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_and_text(pdf_paths, output_folder, text_output_file, pdf_img_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    img_metas = []\n",
    "\n",
    "    for file_index in range(len(pdf_paths)):\n",
    "        pdf_path = pdf_paths[file_index]\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        with open(text_output_file, \"a\", encoding=\"utf-8\") as txt_file:\n",
    "            img_index = 1\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "\n",
    "                extract_text(txt_file, page, file_index, page_num, pdf_img_folder)\n",
    "                img_index, img_metas = extract_images(doc, page_num, page, img_index, file_index, img_metas)\n",
    "\n",
    "    print(f\"\\nText extraction complete. Saved to '{text_output_file}'\")\n",
    "    print(f\"Image extraction complete. Images saved in '{output_folder}'\")\n",
    "\n",
    "    return img_metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output_folder(output_folder)\n",
    "clear_output_folder(pdf_img_folder)\n",
    "img_metas = extract_images_and_text(pdf_paths, output_folder, text_output_file, pdf_img_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Images Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdf_paths)):\n",
    "    pdf_file = pdf_paths[i]\n",
    "    pages = convert_from_path(pdf_file, dpi=200)\n",
    "    for j, page in enumerate(pages):\n",
    "        output_filename = f\"File{i}_Page{j+1}.png\"\n",
    "        page.save(f\"{pdf_img_folder}/{output_filename}\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pdf_pairs = []\n",
    "\n",
    "for i in range(len(img_metas)):\n",
    "    img_meta = img_metas[i]\n",
    "    page = img_meta[1] + 1\n",
    "    file = img_meta[0].split('_')[0]\n",
    "    corresponding_pdf_page = f\"{file}_Page{page}.png\"\n",
    "    corresponding_pdf_txt = f\"{file}_Page{page}.txt\"\n",
    "    img_pdf_pairs.append([img_meta[0], [corresponding_pdf_page], [corresponding_pdf_txt]])\n",
    "\n",
    "img_pdf_pairs[0][1].extend(\n",
    "    sub[1][0] for sub in img_pdf_pairs[1:3]\n",
    ")\n",
    "img_pdf_pairs[0][2].extend(\n",
    "    sub[2][0] for sub in img_pdf_pairs[1:3]\n",
    ")\n",
    "img_pdf_pairs[-1][1].extend(\n",
    "    sub[1][0] for sub in img_pdf_pairs[-4:-2]\n",
    ")\n",
    "img_pdf_pairs[-1][2].extend(\n",
    "    sub[2][0] for sub in img_pdf_pairs[-4:-2]\n",
    ")\n",
    "\n",
    "for i in range(1, len(img_pdf_pairs)-1):\n",
    "    img_pdf_pairs[i][1].extend([\n",
    "    img_pdf_pairs[i-1][1][0],\n",
    "    img_pdf_pairs[i+1][1][0]\n",
    "])\n",
    "    img_pdf_pairs[i][2].extend([\n",
    "    img_pdf_pairs[i-1][2][0],\n",
    "    img_pdf_pairs[i+1][2][0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
    "caption_pairs = []\n",
    "\n",
    "for i in range(len(img_pdf_pairs)):\n",
    "    image_name = img_pdf_pairs[i][0]\n",
    "    image_path = os.path.join(output_folder, image_name)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    pdf_img_paths = []\n",
    "    pdf_img_contexts = img_pdf_pairs[i][1]\n",
    "    for j in range(len(pdf_img_contexts)):\n",
    "        pdf_img = pdf_img_contexts[j]\n",
    "        pdf_img_paths.append(os.path.join(pdf_img_folder, pdf_img))\n",
    "    base64_pdf_imgs = []\n",
    "    for pdf_img_path in pdf_img_paths:\n",
    "        with open(pdf_img_path, \"rb\") as img_file:\n",
    "            base64_pdf_img = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            base64_pdf_imgs.append(base64_pdf_img)\n",
    "\n",
    "    pdf_txt_paths = []\n",
    "    pdf_txt_contexts = img_pdf_pairs[i][2]\n",
    "    for j in range(len(pdf_txt_contexts)):\n",
    "        pdf_txt = pdf_txt_contexts[j]\n",
    "        pdf_txt_paths.append(os.path.join(pdf_img_folder, pdf_txt))\n",
    "    txt_context = \"\"\n",
    "    for pdf_txt_path in pdf_txt_paths:\n",
    "        with open(pdf_txt_path, \"r\") as text_file:\n",
    "            file_text = text_file.read()\n",
    "            txt_context += file_text + \"\\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an image reading expert. Summarize the description \"\n",
    "                \"of the last image given the first three images as context.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Below are three pdf images providing context, followed by a final image extracted from one of these pdfs.\"\n",
    "                        \"Please analyze the three context pdf images, then extract the symptom name and description of the image.\"\n",
    "                        \"The text content from the three pdf images are also provided below for your better understanding.\"\n",
    "                        \"Use the structure of Symptom: xxx, Description: xxx.\"\n",
    "                    )\n",
    "                },\n",
    "                # First context image\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_pdf_imgs[0]}\"\n",
    "                    }\n",
    "                },\n",
    "                # Second context image\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_pdf_imgs[1]}\"\n",
    "                    }\n",
    "                },\n",
    "                # Third context image\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_pdf_imgs[2]}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Here are texts extracted from pdf images above: {txt_content}\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Now here is the final image to summarize:\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "        \n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    summary = response.choices[0].message.content\n",
    "    caption_pairs.append((summary, image_path))\n",
    "    print(f\"----------{image_path}---------\\n\")\n",
    "    print(summary)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "pdf_text = read_text_file(text_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap):\n",
    "    text_splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Total text chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "pdf_chunks = chunk_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "image_embeddings = []\n",
    "for caption_pair in caption_pairs:\n",
    "    image = Image.open(caption_pair[1])\n",
    "\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(image_input)  \n",
    "\n",
    "    image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)\n",
    "    image_embedding = image_embedding.squeeze(0)\n",
    "    image_embeddings.append(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for i in range(len(caption_pairs)):\n",
    "    image_paths.append(caption_pairs[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dense_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for chunk in pdf_chunks:\n",
    "        text_input = clip.tokenize([chunk]).to(device)\n",
    "        text_embedding = model.encode_text(text_input) \n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "        text_embedding = text_embedding.squeeze(0)\n",
    "        text_dense_embeddings.append(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_sparse_embeddings(chunks):\n",
    "    model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
    "    embeddings = list(model.embed(chunks))\n",
    "    return embeddings\n",
    "\n",
    "text_sparse_embeddings = get_text_sparse_embeddings(pdf_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Text Embeddings to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qdrant_client.collection_exists(collection_name=text_collection_name):\n",
    "    qdrant_client.delete_collection(collection_name=text_collection_name)\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=text_collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=len(text_dense_embeddings[1]),\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_dense_embeddings(chunks, embeddings):\n",
    "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "        doc_id = f\"{uuid.uuid4()}\" \n",
    "        qdrant_client.upsert(\n",
    "            collection_name=text_collection_name,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=doc_id,\n",
    "                    payload={\n",
    "                        \"type\": \"text-dense\",\n",
    "                        \"chunk_text\": chunk,\n",
    "                        \"chunk_index\": i\n",
    "                    },\n",
    "                    vector=emb,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "add_text_dense_embeddings(pdf_chunks, text_dense_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_sparse_embeddings(chunks, sparse_embeddings):\n",
    "    for i, (chunk, sparse_vector) in enumerate(zip(chunks, sparse_embeddings)):\n",
    "        doc_id = f\"{uuid.uuid4()}\"\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=text_collection_name,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=doc_id,\n",
    "                    payload={\n",
    "                        \"type\": \"text-sparse\",\n",
    "                        \"chunk_text\": chunk,\n",
    "                        \"chunk_index\": i\n",
    "                    },\n",
    "                    vector={\n",
    "                        \"sparse\": {  \n",
    "                            \"indices\": list(sparse_vector.indices),\n",
    "                            \"values\": list(sparse_vector.values)\n",
    "                        }\n",
    "                    },\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "add_text_sparse_embeddings(pdf_chunks, text_sparse_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qdrant_client.collection_exists(collection_name=image_collection_name):\n",
    "    qdrant_client.delete_collection(collection_name=image_collection_name)\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=image_collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=len(image_embeddings[1]),\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_embeddings(embeddings, image_paths):\n",
    "    for i, (emb, image_path) in enumerate(zip(embeddings, image_paths)):\n",
    "        doc_id = f\"{uuid.uuid4()}\" \n",
    "        qdrant_client.upsert(\n",
    "            collection_name=image_collection_name,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=doc_id,\n",
    "                    payload={\n",
    "                        \"type\": \"images\",\n",
    "                        \"image_index\": i,\n",
    "                        \"image_path\": image_path\n",
    "                    },\n",
    "                    vector=emb,\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_image_embeddings(image_embeddings, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
