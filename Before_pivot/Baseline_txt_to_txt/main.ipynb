{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import chatbot\n",
    "import re\n",
    "import evaluation as eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "query_path = \"query.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respond to a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(\n",
    "        query,\n",
    "        verbose=1):\n",
    "\n",
    "    response = chatbot.disease_list_prediction_without_RAG(query)\n",
    "    pattern = r'(Disease name \\d+:\\s*.*?)(?=Disease name \\d+:|$)'\n",
    "    predictions = re.findall(pattern, response, flags=re.DOTALL)\n",
    "\n",
    "    for prediction in predictions:\n",
    "        if verbose:\n",
    "            print(prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease name 1: Ameloblastoma\n",
      "Disease 1 description: Central, unilocular radiolucent lesion in the mandible with root resorption, tooth displacement, and expansion to the bony cortex.\n",
      "\n",
      "Disease name 2: Odontogenic keratocyst\n",
      "Disease 2 description: Unilocular radiolucent lesion with defined but not corticated borders in the mandible, commonly seen in the incisor region.\n",
      "\n",
      "Disease name 3: Dentigerous cyst\n",
      "Disease 3 description: Unilocular radiolucent lesion associated with impacted teeth, with expansion to the bony cortex and displacement of adjacent teeth.\n"
     ]
    }
   ],
   "source": [
    "query = util.read_query(query_path).replace('\\n', \"\").strip()\n",
    "predictions = respond(query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_respond(\n",
    "        qa_pairs, \n",
    "        prediction_record_path\n",
    "    ):\n",
    "    \n",
    "    truths = []\n",
    "    queries = []\n",
    "    for pair in qa_pairs:\n",
    "        truths.append(pair['answer'])\n",
    "        queries.append(pair['query'])\n",
    "\n",
    "    print(\"Generating responses.\")\n",
    "    prediction_record = []\n",
    "        \n",
    "    for i, query in tqdm(enumerate(queries)):\n",
    "        cleaned_query = query.replace('\\n', \"\")\n",
    "        cleaned_query = cleaned_query.strip()\n",
    "        symptom_prediction = None\n",
    "        disease_names = []\n",
    "        while disease_names == [] or not symptom_prediction:\n",
    "            symptom_prediction = respond(\n",
    "                cleaned_query, \n",
    "                verbose=0\n",
    "            )\n",
    "            for prediction in symptom_prediction:  \n",
    "                match = re.search(r'Disease name \\d+:\\s*(.*?)\\s*Disease \\d+ description:', prediction, re.DOTALL)\n",
    "                if match:\n",
    "                    disease_names.append(match.group(1).strip())\n",
    "\n",
    "        record = {\n",
    "            'index': i,\n",
    "            'original_prediction': symptom_prediction,\n",
    "            'disease_prediction': disease_names,\n",
    "            'true_disease': truths[i]\n",
    "        }\n",
    "\n",
    "        prediction_record.append(record)\n",
    "\n",
    "    util.save_as_json(prediction_record, prediction_record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disease_recall(record_path, sparse_sim_threshold, score_path):\n",
    "    prediction_record = util.load_json(record_path)\n",
    "    total_correct = 0\n",
    "    new_record = []\n",
    "    for entry in tqdm(prediction_record):\n",
    "        disease_truth_list = [entry['true_disease']]\n",
    "        disease_pred = entry['disease_prediction']\n",
    "        correct, sim = eval.embedding_list_hit(disease_truth_list, disease_pred, sparse_sim_threshold)\n",
    "\n",
    "        entry['disease_correct'] = correct\n",
    "        entry['sparse_similarity'] = sim\n",
    "        new_record.append(entry)\n",
    "        \n",
    "        total_correct += correct\n",
    "    \n",
    "    util.save_as_json(new_record, record_path)\n",
    "\n",
    "    if not os.path.exists(score_path):\n",
    "        result = [{'disease_recall': total_correct/len(prediction_record)}]\n",
    "        util.save_as_json(result, score_path)\n",
    "    else:\n",
    "        result = util.load_json(score_path)\n",
    "        result.append([{'disease_recall': total_correct/len(prediction_record)}])\n",
    "        util.save_as_json(result, score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Iteration 0\n",
      "\n",
      "Loaded content from evaluation_dataset/qa_pairs.json\n",
      "Generating responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:57,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record0.json\n",
      "Loaded content from manual_evaluation_results/prediction_record0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record0.json\n",
      "Saved content to manual_evaluation_results/prediction_score0.json\n",
      "Evaluation Iteration 1\n",
      "\n",
      "Loaded content from evaluation_dataset/qa_pairs.json\n",
      "Generating responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:50,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record1.json\n",
      "Loaded content from manual_evaluation_results/prediction_record1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record1.json\n",
      "Saved content to manual_evaluation_results/prediction_score1.json\n",
      "Evaluation Iteration 2\n",
      "\n",
      "Loaded content from evaluation_dataset/qa_pairs.json\n",
      "Generating responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:49,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record2.json\n",
      "Loaded content from manual_evaluation_results/prediction_record2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record2.json\n",
      "Saved content to manual_evaluation_results/prediction_score2.json\n",
      "Evaluation Iteration 3\n",
      "\n",
      "Loaded content from evaluation_dataset/qa_pairs.json\n",
      "Generating responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:52,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record3.json\n",
      "Loaded content from manual_evaluation_results/prediction_record3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record3.json\n",
      "Saved content to manual_evaluation_results/prediction_score3.json\n",
      "Evaluation Iteration 4\n",
      "\n",
      "Loaded content from evaluation_dataset/qa_pairs.json\n",
      "Generating responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:47,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record4.json\n",
      "Loaded content from manual_evaluation_results/prediction_record4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content to manual_evaluation_results/prediction_record4.json\n",
      "Saved content to manual_evaluation_results/prediction_score4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp_num = 5\n",
    "for i in range(exp_num):\n",
    "    print(f'Evaluation Iteration {i}\\n')\n",
    "    qa_pairs = util.load_json('evaluation_dataset/qa_pairs.json')\n",
    "\n",
    "    prediction_record_path = f'manual_evaluation_results/prediction_record{i}.json'\n",
    "\n",
    "    list_respond(\n",
    "        qa_pairs, \n",
    "        prediction_record_path\n",
    "    )\n",
    "\n",
    "    record_path = f'manual_evaluation_results/prediction_record{i}.json'\n",
    "    score_path = f'manual_evaluation_results/prediction_score{i}.json'\n",
    "    disease_recall(record_path, 0.47, score_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
